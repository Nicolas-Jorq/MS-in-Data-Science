{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: Build a CNN for image recognition.\n",
    "\n",
    "### Name: Nicolas Jorquera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I built a Convolutional Neural Network (CNN) from scratch for image recognition. The goal was not just to construct the network, but also to fine-tune the architecture and optimize hyperparameters to achieve high accuracy. I incorporated advanced features like Batch Normalization between Conv and activation layers and utilized Dropout for regularization. I was particularly focused on layer-by-layer construction and didn't rely on pre-built architectures. This experience gave me a deep understanding of how to manually optimize neural networks for specific tasks.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    n = len(y)\n",
    "    one_hot = np.zeros((n, num_class))\n",
    "    one_hot[np.arange(n), y[:, 0]] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "#### Convolutional Layers (Conv2D):\n",
    "This function extracts spatial features and patterns from the images. Here I utilized Three convolutional layers with 32, 64, and 128 filters respectively, using 3×3 kernel sizes. Increasing the depth of the network, should enable it to learn more complex features. The hierarchical pattern learning helps in detecting intricate details in images.\n",
    "\n",
    "#### Batch Normalization Layers (BatchNormalization):\n",
    "This function normalizes the output of the previous layer, accelerating training and improving generalization. Here it was applied after each convolutional layer, before activation. It helps in faster convergence and mitigates the risk of vanishing/exploding gradients; making the model more stable.\n",
    "\n",
    "#### Activation Layers (Activation):\n",
    "This function introduces non-linearity into the network. Here the ReLU (Rectified Linear Unit) activation is used for all hidden layers. It enables the network to model complex non-linear relationships.\n",
    "\n",
    "#### Max Pooling Layers (MaxPooling2D):\n",
    "This function reduces the spatial dimensions, emphasizing the most important features. Here I utilized a 2×2 pooling size, applied after each convolutional layer. It should allow to scale down he feature maps, making the network more robust to variations in the object's appearance.\n",
    "\n",
    "#### Dropout Layers (Dropout):\n",
    "This function is a regularization technique that randomly sets a fraction of the input units to 0 during training. I chose dropout rates of 0.25 for convolutional layers and 0.5 for the dense layer. It should help prevent overfitting by ensuring that the network does not rely too heavily on any specific feature.\n",
    "\n",
    "#### Flatten Layer (Flatten):\n",
    "This function flattens the 2D matrix data into a vector to feed into the dense layers. Here it is applied once before the dense layers. It transforms spatial features into a form suitable for classification; and is necessary between convolutional and dense layers.\n",
    "\n",
    "#### Dense Layers (Dense):\n",
    "Here I utilized one hidden dense layer with 512 units and an output layer with 10 units (for 10 classes). The hidden dense layer further processes the features, and the output layer provides class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 32, 32, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 16, 16, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 8, 8, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1148362 (4.38 MB)\n",
      "Trainable params: 1147914 (4.38 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='s63ame'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 62s 49ms/step - loss: 1.7538 - acc: 0.3759 - val_loss: 1.4084 - val_acc: 0.5001\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 66s 53ms/step - loss: 1.4099 - acc: 0.4964 - val_loss: 1.2614 - val_acc: 0.5536\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 60s 48ms/step - loss: 1.2707 - acc: 0.5522 - val_loss: 1.3035 - val_acc: 0.5619\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 68s 55ms/step - loss: 1.1766 - acc: 0.5936 - val_loss: 1.2018 - val_acc: 0.5757\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 65s 52ms/step - loss: 1.0954 - acc: 0.6253 - val_loss: 1.0887 - val_acc: 0.6168\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 64s 51ms/step - loss: 1.0515 - acc: 0.6444 - val_loss: 1.0054 - val_acc: 0.6616\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 61s 49ms/step - loss: 1.0261 - acc: 0.6597 - val_loss: 0.8781 - val_acc: 0.7068\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 61s 49ms/step - loss: 0.9984 - acc: 0.6671 - val_loss: 1.1926 - val_acc: 0.6250\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 65s 52ms/step - loss: 0.9826 - acc: 0.6740 - val_loss: 0.8734 - val_acc: 0.7130\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 59s 47ms/step - loss: 0.9716 - acc: 0.6841 - val_loss: 0.9477 - val_acc: 0.6876\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 57s 36ms/step - loss: 0.9982 - acc: 0.6702 - val_loss: 1.0242 - val_acc: 0.6528\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.9848 - acc: 0.6761 - val_loss: 0.9541 - val_acc: 0.6798\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.9785 - acc: 0.6809 - val_loss: 0.9046 - val_acc: 0.6831\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.9759 - acc: 0.6817 - val_loss: 1.1605 - val_acc: 0.6409\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 65s 42ms/step - loss: 0.9848 - acc: 0.6844 - val_loss: 0.7799 - val_acc: 0.7435\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.9914 - acc: 0.6798 - val_loss: 0.8538 - val_acc: 0.7409\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 61s 39ms/step - loss: 0.9926 - acc: 0.6817 - val_loss: 1.0052 - val_acc: 0.6875\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 66s 42ms/step - loss: 0.9899 - acc: 0.6812 - val_loss: 1.2172 - val_acc: 0.6300\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 62s 40ms/step - loss: 0.9841 - acc: 0.6847 - val_loss: 0.8509 - val_acc: 0.7364\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 0.9915 - acc: 0.6812 - val_loss: 1.0470 - val_acc: 0.6687\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train_vec, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 1.1145 - acc: 0.6394\n",
      "loss = 1.114530324935913\n",
      "accuracy = 0.6394000053405762\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experimenting on Improving the Model further using more Advanced Techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 32, 32, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 32, 32, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 16, 16, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 16, 16, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 16, 16, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 8, 8, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 8, 8, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 8, 8, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 4, 4, 128)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1343018 (5.12 MB)\n",
      "Trainable params: 1342122 (5.12 MB)\n",
      "Non-trainable params: 896 (3.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning Rate Reduction\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "625/625 [==============================] - 96s 150ms/step - loss: 1.8311 - accuracy: 0.3366 - val_loss: 857.1834 - val_accuracy: 0.1200 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "625/625 [==============================] - 95s 152ms/step - loss: 1.4945 - accuracy: 0.4551 - val_loss: 562.8972 - val_accuracy: 0.1567 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "625/625 [==============================] - 99s 159ms/step - loss: 1.3259 - accuracy: 0.5223 - val_loss: 615.9886 - val_accuracy: 0.1049 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "625/625 [==============================] - 97s 156ms/step - loss: 1.2048 - accuracy: 0.5740 - val_loss: 341.0070 - val_accuracy: 0.1333 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.1213 - accuracy: 0.6073\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 1.1213 - accuracy: 0.6073 - val_loss: 379.1577 - val_accuracy: 0.1344 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "625/625 [==============================] - 97s 155ms/step - loss: 0.9978 - accuracy: 0.6531 - val_loss: 237.6071 - val_accuracy: 0.1517 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.9601 - accuracy: 0.6639 - val_loss: 174.1246 - val_accuracy: 0.1906 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "625/625 [==============================] - 98s 157ms/step - loss: 0.9225 - accuracy: 0.6795 - val_loss: 197.6238 - val_accuracy: 0.1562 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "625/625 [==============================] - 100s 159ms/step - loss: 0.9021 - accuracy: 0.6871 - val_loss: 253.2245 - val_accuracy: 0.1397 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "625/625 [==============================] - 99s 158ms/step - loss: 0.8663 - accuracy: 0.7016 - val_loss: 89.9519 - val_accuracy: 0.3137 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "625/625 [==============================] - 98s 156ms/step - loss: 0.8510 - accuracy: 0.7089 - val_loss: 55.3054 - val_accuracy: 0.3150 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "625/625 [==============================] - 91s 146ms/step - loss: 0.8259 - accuracy: 0.7174 - val_loss: 62.5825 - val_accuracy: 0.3539 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.8023 - accuracy: 0.7254 - val_loss: 217.9763 - val_accuracy: 0.2175 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "625/625 [==============================] - 99s 158ms/step - loss: 0.7796 - accuracy: 0.7350 - val_loss: 75.0287 - val_accuracy: 0.3033 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.7735 - accuracy: 0.7369\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "625/625 [==============================] - 100s 160ms/step - loss: 0.7735 - accuracy: 0.7369 - val_loss: 54.9217 - val_accuracy: 0.3207 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "625/625 [==============================] - 100s 159ms/step - loss: 0.7186 - accuracy: 0.7556 - val_loss: 77.3444 - val_accuracy: 0.2932 - lr: 2.5000e-04\n",
      "Epoch 17/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.7105 - accuracy: 0.7603 - val_loss: 63.8642 - val_accuracy: 0.3734 - lr: 2.5000e-04\n",
      "Epoch 18/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.6974 - accuracy: 0.7628 - val_loss: 94.0139 - val_accuracy: 0.2944 - lr: 2.5000e-04\n",
      "Epoch 19/50\n",
      "625/625 [==============================] - 101s 161ms/step - loss: 0.6889 - accuracy: 0.7649 - val_loss: 111.3650 - val_accuracy: 0.2668 - lr: 2.5000e-04\n",
      "Epoch 20/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.6788 - accuracy: 0.7701\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "625/625 [==============================] - 103s 165ms/step - loss: 0.6788 - accuracy: 0.7701 - val_loss: 64.0735 - val_accuracy: 0.3311 - lr: 2.5000e-04\n",
      "Epoch 21/50\n",
      "625/625 [==============================] - 100s 160ms/step - loss: 0.6525 - accuracy: 0.7783 - val_loss: 74.7272 - val_accuracy: 0.3111 - lr: 1.2500e-04\n",
      "Epoch 22/50\n",
      "625/625 [==============================] - 106s 169ms/step - loss: 0.6495 - accuracy: 0.7800 - val_loss: 76.8617 - val_accuracy: 0.3176 - lr: 1.2500e-04\n",
      "Epoch 23/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.6414 - accuracy: 0.7822\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "625/625 [==============================] - 101s 161ms/step - loss: 0.6414 - accuracy: 0.7822 - val_loss: 84.7627 - val_accuracy: 0.3167 - lr: 1.2500e-04\n",
      "Epoch 24/50\n",
      "625/625 [==============================] - 103s 165ms/step - loss: 0.6164 - accuracy: 0.7886 - val_loss: 87.9607 - val_accuracy: 0.3079 - lr: 6.2500e-05\n",
      "Epoch 25/50\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.6202 - accuracy: 0.7897 - val_loss: 99.1559 - val_accuracy: 0.2926 - lr: 6.2500e-05\n",
      "Epoch 26/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.7912\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.6139 - accuracy: 0.7912 - val_loss: 91.0346 - val_accuracy: 0.3009 - lr: 6.2500e-05\n",
      "Epoch 27/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.6058 - accuracy: 0.7928 - val_loss: 94.3832 - val_accuracy: 0.2985 - lr: 3.1250e-05\n",
      "Epoch 28/50\n",
      "625/625 [==============================] - 98s 157ms/step - loss: 0.6020 - accuracy: 0.7964 - val_loss: 76.9636 - val_accuracy: 0.3163 - lr: 3.1250e-05\n",
      "Epoch 29/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.6071 - accuracy: 0.7940\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "625/625 [==============================] - 97s 155ms/step - loss: 0.6071 - accuracy: 0.7940 - val_loss: 71.7646 - val_accuracy: 0.3306 - lr: 3.1250e-05\n",
      "Epoch 30/50\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.6029 - accuracy: 0.7948 - val_loss: 81.5026 - val_accuracy: 0.3227 - lr: 1.5625e-05\n",
      "Epoch 31/50\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.6038 - accuracy: 0.7948 - val_loss: 87.5217 - val_accuracy: 0.3164 - lr: 1.5625e-05\n",
      "Epoch 32/50\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7955\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "625/625 [==============================] - 96s 154ms/step - loss: 0.6000 - accuracy: 0.7955 - val_loss: 78.3954 - val_accuracy: 0.3287 - lr: 1.5625e-05\n",
      "Epoch 33/50\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.5993 - accuracy: 0.7968 - val_loss: 79.5667 - val_accuracy: 0.3286 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.5946 - accuracy: 0.7972 - val_loss: 78.2697 - val_accuracy: 0.3266 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.5967 - accuracy: 0.7976 - val_loss: 82.6972 - val_accuracy: 0.3188 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.5920 - accuracy: 0.7987 - val_loss: 77.9671 - val_accuracy: 0.3246 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.5983 - accuracy: 0.7965 - val_loss: 78.2101 - val_accuracy: 0.3278 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.5917 - accuracy: 0.7981 - val_loss: 82.0889 - val_accuracy: 0.3227 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.5883 - accuracy: 0.8005 - val_loss: 84.2009 - val_accuracy: 0.3204 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.5897 - accuracy: 0.8000 - val_loss: 90.5409 - val_accuracy: 0.3144 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.5897 - accuracy: 0.8005 - val_loss: 76.9239 - val_accuracy: 0.3259 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.5935 - accuracy: 0.7985 - val_loss: 82.8799 - val_accuracy: 0.3203 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.5913 - accuracy: 0.7975 - val_loss: 84.2659 - val_accuracy: 0.3227 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.5954 - accuracy: 0.7973 - val_loss: 87.4238 - val_accuracy: 0.3144 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.5941 - accuracy: 0.7989 - val_loss: 85.1321 - val_accuracy: 0.3167 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "625/625 [==============================] - 93s 150ms/step - loss: 0.5865 - accuracy: 0.7975 - val_loss: 94.0577 - val_accuracy: 0.3078 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.5864 - accuracy: 0.7991 - val_loss: 92.9876 - val_accuracy: 0.3114 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.5903 - accuracy: 0.7982 - val_loss: 91.8025 - val_accuracy: 0.3128 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.5871 - accuracy: 0.8016 - val_loss: 101.2378 - val_accuracy: 0.3023 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.5887 - accuracy: 0.8000 - val_loss: 90.2508 - val_accuracy: 0.3137 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "history = model.fit(datagen.flow(x_tr, y_tr, batch_size=batch_size),\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 16ms/step - loss: 89.3712 - accuracy: 0.3094\n",
      "loss = 89.37116241455078\n",
      "accuracy = 0.3093999922275543\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "- I was curious to see how increasing the number of epochs, augmenting the data to add variability; and adding a Learning Rate Reduction would influence the model. While testing the model with the training data the model performed a lot better; achieving an accuracy of 0.8. However; clearly this model was overfitted since at epoch 25 the model's accuracy stopped improving. This was seen when the model was tested using the test data; and it only achieved an accuracy of 0.3. Below I will run the same model with only 25 epochs to see if the model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "625/625 [==============================] - 106s 167ms/step - loss: 1.8348 - accuracy: 0.3380 - val_loss: 319.5312 - val_accuracy: 0.1758 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "625/625 [==============================] - 102s 162ms/step - loss: 1.4792 - accuracy: 0.4641 - val_loss: 250.9285 - val_accuracy: 0.1665 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "625/625 [==============================] - 106s 169ms/step - loss: 1.3251 - accuracy: 0.5250 - val_loss: 324.6452 - val_accuracy: 0.1516 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.2073 - accuracy: 0.5726\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "625/625 [==============================] - 97s 156ms/step - loss: 1.2073 - accuracy: 0.5726 - val_loss: 362.2020 - val_accuracy: 0.1406 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "625/625 [==============================] - 103s 164ms/step - loss: 1.0774 - accuracy: 0.6197 - val_loss: 161.1862 - val_accuracy: 0.2567 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "625/625 [==============================] - 113s 181ms/step - loss: 1.0236 - accuracy: 0.6425 - val_loss: 230.8694 - val_accuracy: 0.2128 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "625/625 [==============================] - 132s 211ms/step - loss: 0.9773 - accuracy: 0.6594 - val_loss: 258.2298 - val_accuracy: 0.1755 - lr: 5.0000e-04\n",
      "Epoch 8/25\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.9452 - accuracy: 0.6726\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "625/625 [==============================] - 128s 205ms/step - loss: 0.9452 - accuracy: 0.6726 - val_loss: 181.2883 - val_accuracy: 0.2027 - lr: 5.0000e-04\n",
      "Epoch 9/25\n",
      "625/625 [==============================] - 126s 202ms/step - loss: 0.8864 - accuracy: 0.6938 - val_loss: 215.2502 - val_accuracy: 0.1724 - lr: 2.5000e-04\n",
      "Epoch 10/25\n",
      "625/625 [==============================] - 128s 204ms/step - loss: 0.8632 - accuracy: 0.7023 - val_loss: 161.4319 - val_accuracy: 0.2131 - lr: 2.5000e-04\n",
      "Epoch 11/25\n",
      "625/625 [==============================] - 121s 194ms/step - loss: 0.8435 - accuracy: 0.7107 - val_loss: 137.0977 - val_accuracy: 0.2589 - lr: 2.5000e-04\n",
      "Epoch 12/25\n",
      "625/625 [==============================] - 99s 158ms/step - loss: 0.8341 - accuracy: 0.7103 - val_loss: 133.6364 - val_accuracy: 0.2625 - lr: 2.5000e-04\n",
      "Epoch 13/25\n",
      "625/625 [==============================] - 102s 164ms/step - loss: 0.8083 - accuracy: 0.7219 - val_loss: 86.2439 - val_accuracy: 0.3420 - lr: 2.5000e-04\n",
      "Epoch 14/25\n",
      "625/625 [==============================] - 102s 163ms/step - loss: 0.7964 - accuracy: 0.7263 - val_loss: 87.5985 - val_accuracy: 0.3462 - lr: 2.5000e-04\n",
      "Epoch 15/25\n",
      "625/625 [==============================] - 102s 163ms/step - loss: 0.7777 - accuracy: 0.7304 - val_loss: 128.4496 - val_accuracy: 0.2703 - lr: 2.5000e-04\n",
      "Epoch 16/25\n",
      "625/625 [==============================] - 99s 159ms/step - loss: 0.7771 - accuracy: 0.7335 - val_loss: 87.8282 - val_accuracy: 0.3235 - lr: 2.5000e-04\n",
      "Epoch 17/25\n",
      "625/625 [==============================] - 101s 161ms/step - loss: 0.7579 - accuracy: 0.7402 - val_loss: 86.3131 - val_accuracy: 0.3737 - lr: 2.5000e-04\n",
      "Epoch 18/25\n",
      "625/625 [==============================] - 103s 165ms/step - loss: 0.7487 - accuracy: 0.7442 - val_loss: 90.3287 - val_accuracy: 0.3099 - lr: 2.5000e-04\n",
      "Epoch 19/25\n",
      "625/625 [==============================] - 124s 199ms/step - loss: 0.7408 - accuracy: 0.7430 - val_loss: 52.6304 - val_accuracy: 0.4213 - lr: 2.5000e-04\n",
      "Epoch 20/25\n",
      "625/625 [==============================] - 143s 229ms/step - loss: 0.7292 - accuracy: 0.7502 - val_loss: 86.0139 - val_accuracy: 0.3623 - lr: 2.5000e-04\n",
      "Epoch 21/25\n",
      "625/625 [==============================] - 137s 219ms/step - loss: 0.7201 - accuracy: 0.7541 - val_loss: 97.8814 - val_accuracy: 0.3252 - lr: 2.5000e-04\n",
      "Epoch 22/25\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.7095 - accuracy: 0.7567\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "625/625 [==============================] - 142s 228ms/step - loss: 0.7095 - accuracy: 0.7567 - val_loss: 62.0718 - val_accuracy: 0.4190 - lr: 2.5000e-04\n",
      "Epoch 23/25\n",
      "625/625 [==============================] - 138s 220ms/step - loss: 0.6812 - accuracy: 0.7674 - val_loss: 76.5734 - val_accuracy: 0.3699 - lr: 1.2500e-04\n",
      "Epoch 24/25\n",
      "625/625 [==============================] - 133s 213ms/step - loss: 0.6695 - accuracy: 0.7717 - val_loss: 67.4946 - val_accuracy: 0.3929 - lr: 1.2500e-04\n",
      "Epoch 25/25\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.6567 - accuracy: 0.7748\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.6567 - accuracy: 0.7748 - val_loss: 87.7602 - val_accuracy: 0.3830 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "history = model.fit(datagen.flow(x_tr, y_tr, batch_size=batch_size),\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[learning_rate_reduction])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
