{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac451fd",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993ce65",
   "metadata": {},
   "source": [
    "## 1. Neurons:\n",
    "A neuron is a fundamental building block of a neural network, and it computes a weighted sum of its input plus a bias term, then passes it through an activation function.\n",
    "\n",
    "**Equation:**\n",
    "A single neuron's output can be represented as:\n",
    "<h4><center>$ y = f\\left( \\sum_{i=1}^{n} w_i \\cdot x_i + b \\right) $</center></h4>\n",
    "\n",
    "where:\n",
    "- $y$: output of the neuron\n",
    "- $w_i$: weight of the i-th input\n",
    "- $x_i$: i-th input\n",
    "- $b$: bias term\n",
    "- $f$: activation function, such as ReLU or Sigmoid\n",
    "- $n$: number of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7a2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7618a",
   "metadata": {},
   "source": [
    "#### Sigmoid Function\n",
    "The mathematical expression for the sigmoid function is given by:\n",
    "\n",
    "<h4><center>$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $</center></h4>\n",
    "\n",
    "##### Properties and Applications:\n",
    "1. **Range**: The sigmoid function takes any real-valued number and squashes it into the range between 0 and 1. The output can be interpreted as a probability, making it useful for binary classification tasks.\n",
    "\n",
    "2. **Smooth Gradient**: The function is smooth, and its derivative is easy to compute. The gradient is given by:\n",
    "\n",
    "   <h4><center>$ \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) $</center></h4>\n",
    "\n",
    "   The smooth gradient helps in gradient-based optimization methods like Gradient Descent.\n",
    "\n",
    "3. **Activation in Hidden Layers**: Earlier in neural network history, the sigmoid function was commonly used in hidden layers to introduce non-linear properties to the model. The non-linearity allows the network to learn complex mappings from inputs to outputs.\n",
    "\n",
    "4. **Vanishing Gradient Problem**: One downside of the sigmoid function is that its gradient becomes very small for very large or very small input values. This leads to the so-called \"vanishing gradient\" problem during training, where the gradients become so small that the weights hardly update, slowing down or even halting the learning process.\n",
    "\n",
    "5. **Computational Efficiency**: The sigmoid function is computationally more expensive compared to other activation functions like ReLU. The exponential function involved in its computation can be costly, especially in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14af272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_neuron(x, w, b):\n",
    "    weighted_sum = np.dot(x, w) + b\n",
    "    return sigmoid(weighted_sum)\n",
    "\n",
    "# Inputs, Weights, and Bias\n",
    "x = np.array([0.5, 0.3, 0.1])\n",
    "w = np.array([0.4, 0.2, 0.6])\n",
    "b = 0.1\n",
    "\n",
    "# Output of the Neuron\n",
    "output = single_neuron(x, w, b)\n",
    "print(f\"Output of the neuron: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843fbb7",
   "metadata": {},
   "source": [
    "In the **Single Neuron function**  `x` is the input vector, `w` is the weights vector, and `b` is the bias. The function calculates the weighted sum of the inputs and passes it through the sigmoid activation function. The `np.dot` function computes the dot product of the input and weights, effectively implementing the equation $ y = f\\left( \\sum_{i=1}^{n} w_i \\cdot x_i + b \\right) $, where $ f $ is the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57d463",
   "metadata": {},
   "source": [
    "## 2. Weights:\n",
    "Weights are parameters within the neural network that are fine-tuned during training. They define the strength of connections between neurons in different layers.\n",
    "\n",
    "**Importance:**\n",
    "Weights allow the model to generalize from the training data to unseen data. By adjusting the weights, the network minimizes the error between the predicted and actual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f5c1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [0.39513082 0.19707849 0.59902616]\n",
      "Updated bias: 0.09026164910025532\n"
     ]
    }
   ],
   "source": [
    "def update_weights(x, y_true, w, b, learning_rate=0.1):\n",
    "    # Predicted Output\n",
    "    y_pred = single_neuron(x, w, b)\n",
    "    \n",
    "    # Error Derivative\n",
    "    d_error = 2 * (y_pred - y_true)\n",
    "    \n",
    "    # Derivative with respect to Weights and Bias\n",
    "    d_weights = d_error * x * y_pred * (1 - y_pred)\n",
    "    d_bias = d_error * y_pred * (1 - y_pred)\n",
    "    \n",
    "    # Update Weights and Bias\n",
    "    w -= learning_rate * d_weights\n",
    "    b -= learning_rate * d_bias\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Target Output\n",
    "y_true = 0.4\n",
    "\n",
    "# Updated Weights and Bias\n",
    "new_w, new_b = update_weights(x, y_true, w, b)\n",
    "print(f\"Updated weights: {new_w}\")\n",
    "print(f\"Updated bias: {new_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b95a6",
   "metadata": {},
   "source": [
    "The **Update Weights function** updates the weights and bias based on the error between the predicted and true output.\n",
    "- `y_pred` is the output from the neuron.\n",
    "- `y_true` is the actual target output.\n",
    "- `d_error` computes the derivative of the error with respect to the output, using a simple mean squared error. \n",
    "- `d_weights` and `d_bias` calculate the gradients for the weights and bias, respectively, considering the sigmoid activation function's derivative $ y'(x) = y(x)(1 - y(x)) $.\n",
    "- The weights and bias are updated by subtracting a fraction (controlled by `learning_rate`) of the computed gradients. This helps in reducing the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebcf3f",
   "metadata": {},
   "source": [
    "## 3. Convolutional Layer:\n",
    "A convolutional layer applies a convolution operation to its input, helping the network to automatically and adaptively learn spatial hierarchies of features.\n",
    "\n",
    "1. **Filters/Kernels**: Convolutional layers use a set of learnable filters or kernels. Each kernel is small spatially but extends through the full depth of the input volume. These filters are applied to the input to produce feature maps, emphasizing certain features like edges or textures.\n",
    "\n",
    "2. **Sliding Window**: The kernels slide across the input data (such as an image) to produce a feature map. This sliding window operation is the convolution.\n",
    "\n",
    "3. **Stride**: This defines how much the filter moves at each step of the sliding window. A stride of 1 moves the filter one pixel at a time, while a stride of 2 moves it two pixels, etc.\n",
    "\n",
    "4. **Padding**: Padding adds extra pixels around the input image. It controls the spatial dimensions of the output volumes (for example, to keep them the same as the input).\n",
    "\n",
    "5. **Pooling**: While not part of the convolution operation itself, pooling layers often follow convolutional layers to reduce the spatial dimensions of the data.\n",
    "\n",
    "**Equation:**\n",
    "The convolution operation can be represented as:\n",
    "<h4><center>$ (f * g)(t) = \\sum_{\\tau=-\\infty}^{\\infty} f(\\tau) \\cdot g(t - \\tau) $</center></h4>\n",
    "\n",
    "where:\n",
    "- $f$: input feature map\n",
    "- $g$: kernel or filter\n",
    "- $t$: location in the output feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de9bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution2D(image, kernel, stride=1, padding=0):\n",
    "    # Add padding to the image\n",
    "    image_padded = np.pad(image, padding, mode='constant')\n",
    "    \n",
    "    # Dimensions of the output feature map\n",
    "    output_shape = ((image.shape[0] - kernel.shape[0] + 2 * padding) // stride + 1,\n",
    "                    (image.shape[1] - kernel.shape[1] + 2 * padding) // stride + 1)\n",
    "    \n",
    "    # Initialize the output feature map\n",
    "    feature_map = np.zeros(output_shape)\n",
    "    \n",
    "    # Apply the kernel to the image\n",
    "    for i in range(0, output_shape[0]):\n",
    "        for j in range(0, output_shape[1]):\n",
    "            x_start = i * stride\n",
    "            x_end = x_start + kernel.shape[0]\n",
    "            y_start = j * stride\n",
    "            y_end = y_start + kernel.shape[1]\n",
    "            \n",
    "            # Element-wise multiplication of the kernel and the image patch\n",
    "            feature_map[i, j] = np.sum(image_padded[x_start:x_end, y_start:y_end] * kernel)\n",
    "    \n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "966f02b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8. 8. 8.]\n",
      " [8. 8. 8.]\n",
      " [8. 8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "image = np.array([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12],\n",
    "                  [13, 14, 15, 16]])\n",
    "\n",
    "# Define a kernel, for example, a 2x2 kernel for edge detection\n",
    "kernel = np.array([[-1, -1],\n",
    "                   [1, 1]])\n",
    "\n",
    "# Call the convolution2D function\n",
    "convolution2D_output = convolution2D(image, kernel)\n",
    "\n",
    "# Print the output\n",
    "print(convolution2D_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495d707",
   "metadata": {},
   "source": [
    "- **Padding**: The function begins by adding padding to the image using `np.pad` if necessary.\n",
    "\n",
    "- **Output Shape Calculation**: The dimensions of the output feature map are calculated based on the input size, kernel size, stride, and padding.\n",
    "\n",
    "- **Convolution Operation**: The nested loops slide the kernel across the padded image with the given stride. For each position, a patch of the image is multiplied element-wise with the kernel, and the result is summed up to form a single pixel in the output feature map.\n",
    "\n",
    "- **Result**: The function returns the feature map representing the convoluted layer's output.\n",
    "\n",
    "The example kernel provided is a very simple one that might detect horizontal edges. Below we will explore how these different concpets can be combined into a more functional model.\n",
    "\n",
    "***Significance:*** Convolutional layers form the core of CNNs, enabling them to learn spatial features from images in a hierarchical and translation-invariant manner. The basic concept of applying a filter across the image captures essential features, enabling deep learning models to perform complex tasks in computer vision, such as object detection, image segmentation, and face recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41ddf5",
   "metadata": {},
   "source": [
    "## 4. Pooling Layer:\n",
    "Pooling layers reduce the dimensionality of the data, retaining only essential information. \n",
    "1. **Function**: Pooling reduces the spatial size of the representation, reducing the number of parameters and computation in the network. This helps to control overfitting.\n",
    "\n",
    "2. **Types**: The two most common types of pooling are Max Pooling and Average Pooling.\n",
    "   - **Max Pooling**: Selects the maximum value from each group of values in a local region of the input.\n",
    "   - **Average Pooling**: Calculates the average value for each group of values in a local region of the input.\n",
    "\n",
    "3. **Window Size**: The size of the window determines how many pixels are grouped together for each pooling operation.\n",
    "\n",
    "4. **Stride**: Like in convolution, stride controls how the window moves across the input.\n",
    "\n",
    "5. **No Learnable Parameters**: Unlike convolutional layers, pooling operations have no learnable parameters, meaning they do not change during the training process.\n",
    "\n",
    "**Equation for Max Pooling:**\n",
    "If you have a $2 \\times 2$ pooling window, the max pooling operation selects the maximum value from the $2 \\times 2$ grid:\n",
    "<h4><center>$ \\text{Max} \\left( a, b, c, d \\right)$</center></h4>\n",
    "\n",
    "where $a, b, c,$ and $d$ are the values in the $2 \\times 2$ window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b80869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling2D(image, pool_size=2, stride=2):\n",
    "    # Dimensions of the output\n",
    "    output_shape = ((image.shape[0] - pool_size) // stride + 1,\n",
    "                    (image.shape[1] - pool_size) // stride + 1)\n",
    "\n",
    "    # Initialize the output\n",
    "    pooled_output = np.zeros(output_shape)\n",
    "\n",
    "    # Apply max pooling\n",
    "    for i in range(0, output_shape[0]):\n",
    "        for j in range(0, output_shape[1]):\n",
    "            x_start = i * stride\n",
    "            x_end = x_start + pool_size\n",
    "            y_start = j * stride\n",
    "            y_end = y_start + pool_size\n",
    "            \n",
    "            # Take the max value from the window\n",
    "            pooled_output[i, j] = np.max(image[x_start:x_end, y_start:y_end])\n",
    "\n",
    "    return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d668dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  8.]\n",
      " [14. 16.]]\n"
     ]
    }
   ],
   "source": [
    "image = np.array([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12],\n",
    "                  [13, 14, 15, 16]])\n",
    "\n",
    "pooled_output = max_pooling2D(image)\n",
    "print(pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a86e2e",
   "metadata": {},
   "source": [
    "- **Output Shape Calculation**: Based on the input size, pool size, and stride, the dimensions of the output are calculated.\n",
    "\n",
    "- **Max Pooling Operation**: The nested loops slide the pooling window across the image with the given stride. For each position, the maximum value within the window is taken and assigned to the corresponding position in the output.\n",
    "\n",
    "- **Result**: The function returns the pooled output, which is a reduced-size version of the input, retaining only the most prominent features.\n",
    "\n",
    "***Significance:*** Pooling layers are essential in CNNs for down-sampling the spatial dimensions, reducing the computational complexity, and helping to make the detection of features invariant to scale and orientation changes. Max pooling, in particular, highlights the most salient aspects of the features, thus preserving essential information while discarding redundant details. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f089b",
   "metadata": {},
   "source": [
    "## 5. Activation Function:\n",
    "Activation functions introduce non-linearity to the model, allowing it to learn complex patterns. Common activation functions include ReLU and Sigmoid.\n",
    "\n",
    "**Equations:**\n",
    "- **ReLU:** $ f(x) = \\max(0, x) $\n",
    "- **Sigmoid:** $ f(x) = \\frac{1}{1 + e^{-x}} $\n",
    "\n",
    "### 6. Backpropagation:\n",
    "Backpropagation is the algorithm used to update the weights in the network. It computes the gradient of the loss function with respect to each weight by applying the chain rule.\n",
    "\n",
    "**Equation:**\n",
    "The weight update can be represented as:\n",
    "<h4><center>$ w_i = w_i - \\alpha \\frac{\\partial L}{\\partial w_i} $</center></h4>\n",
    "\n",
    "where:\n",
    "- $w_i$: weight of the i-th input\n",
    "- $\\alpha$: learning rate\n",
    "- $L$: loss function\n",
    "\n",
    "### Relevance to Data Science:\n",
    "These concepts lead to powerful models that can capture complex patterns and relationships in data. The ability to build hierarchical representations through convolutional and pooling layers makes CNNs particularly effective for image analysis. They have revolutionized areas like image recognition, video analysis, and even non-visual tasks like natural language processing. By understanding the underlying mathematics and theories, data scientists can better utilize these tools, design more efficient models, and innovate in various applications.\n",
    "\n",
    "In data science, understanding the theoretical components enables the practitioner to make informed decisions about model architecture, optimization, and evaluation. It bridges the gap between mathematical abstractions and real-world applications, enhancing both the interpretability and effectiveness of the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
