# Numerical Linear Algebra for Big Data

## Numerical Linear Algebra Foundations
### Matrix Operations & Decomposition
- Covers essential matrix operations including addition, multiplication, and inversion, which serve as the backbone for various machine learning algorithms and data manipulation tasks.
- Highlights decomposition techniques such as Singular Value Decomposition (SVD) and QR Decomposition that are crucial for solving linear equations, optimizing algorithms, and feature selection in data science projects.

### Dimensionality Reduction & Clustering Techniques
- Focuses on Principal Component Analysis (PCA) for reducing the dimensionality of datasets, which is instrumental in data visualization, noise reduction, and improving machine learning model performance.
- Discusses Non-negative Matrix Factorization (NMF) as another approach for dimensionality reduction and clustering, commonly applied in fields like text mining and image processing for generating more interpretable components.

## SVD and PCA in Recommender Systems
### Data Overview & Dimensionality Reduction
- Explored datasets on comic book reviews and book information to demonstrate dimensionality reduction techniques. Utilized Singular Value Decomposition (SVD) and Principal Component Analysis (PCA) for this purpose.

### Data Processing & SVD with Cosine Similarity
- Imposed restrictions on review counts and created a sparse matrix for analysis. Applied SVD and Cosine Similarity to measure how similar books are in a multi-dimensional space.

### SVD via Sci-kit Learn & Linear Regression
- Leveraged sci-kit learn for a different SVD implementation, focusing on dimensionality reduction. Created a Linear Regression model using Truncated SVD text reviews and ratings.

### PCA Analysis & Conclusion
- Reduced dataset into two principal components for pattern identification. Concluded by discussing algorithm limitations and the role of linear algebra in machine learning.
